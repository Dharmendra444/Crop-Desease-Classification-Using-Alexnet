{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Basic_Colab.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"3crDr6VbAv-7","colab_type":"code","outputId":"7e21d797-5ef6-436e-c91f-7ae560233c1a","executionInfo":{"status":"ok","timestamp":1555473624791,"user_tz":-330,"elapsed":5355,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["!git clone https://github.com/saroz014/Plant-Diseases-Recognition.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'Plant-Diseases-Recognition'...\n","remote: Enumerating objects: 213, done.\u001b[K\n","remote: Counting objects:   0% (1/213)   \u001b[K\rremote: Counting objects:   1% (3/213)   \u001b[K\rremote: Counting objects:   2% (5/213)   \u001b[K\rremote: Counting objects:   3% (7/213)   \u001b[K\rremote: Counting objects:   4% (9/213)   \u001b[K\rremote: Counting objects:   5% (11/213)   \u001b[K\rremote: Counting objects:   6% (13/213)   \u001b[K\rremote: Counting objects:   7% (15/213)   \u001b[K\rremote: Counting objects:   8% (18/213)   \u001b[K\rremote: Counting objects:   9% (20/213)   \u001b[K\rremote: Counting objects:  10% (22/213)   \u001b[K\rremote: Counting objects:  11% (24/213)   \u001b[K\rremote: Counting objects:  12% (26/213)   \u001b[K\rremote: Counting objects:  13% (28/213)   \u001b[K\rremote: Counting objects:  14% (30/213)   \u001b[K\rremote: Counting objects:  15% (32/213)   \u001b[K\rremote: Counting objects:  16% (35/213)   \rremote: Counting objects:  17% (37/213)   \u001b[K\rremote: Counting objects:  18% (39/213)   \u001b[K\rremote: Counting objects:  19% (41/213)   \u001b[K\rremote: Counting objects:  20% (43/213)   \u001b[K\rremote: Counting objects:  21% (45/213)   \u001b[K\rremote: Counting objects:  22% (47/213)   \u001b[K\rremote: Counting objects:  23% (49/213)   \u001b[K\rremote: Counting objects:  24% (52/213)   \u001b[K\rremote: Counting objects:  25% (54/213)   \u001b[K\rremote: Counting objects:  26% (56/213)   \u001b[K\rremote: Counting objects:  27% (58/213)   \u001b[K\rremote: Counting objects:  28% (60/213)   \u001b[K\rremote: Counting objects:  29% (62/213)   \u001b[K\rremote: Counting objects:  30% (64/213)   \u001b[K\rremote: Counting objects:  31% (67/213)   \rremote: Counting objects:  32% (69/213)   \u001b[K\rremote: Counting objects:  33% (71/213)   \u001b[K\rremote: Counting objects:  34% (73/213)   \u001b[K\rremote: Counting objects:  35% (75/213)   \u001b[K\rremote: Counting objects:  36% (77/213)   \u001b[K\rremote: Counting objects:  37% (79/213)   \u001b[K\rremote: Counting objects:  38% (81/213)   \u001b[K\rremote: Counting objects:  39% (84/213)   \u001b[K\rremote: Counting objects:  40% (86/213)   \u001b[K\rremote: Counting objects:  41% (88/213)   \u001b[K\rremote: Counting objects:  42% (90/213)   \u001b[K\rremote: Counting objects:  43% (92/213)   \u001b[K\rremote: Counting objects:  44% (94/213)   \u001b[K\rremote: Counting objects:  45% (96/213)   \u001b[K\rremote: Counting objects:  46% (98/213)   \rremote: Counting objects:  47% (101/213)   \u001b[K\rremote: Counting objects:  48% (103/213)   \u001b[K\rremote: Counting objects:  49% (105/213)   \u001b[K\rremote: Counting objects:  50% (107/213)   \u001b[K\rremote: Counting objects:  51% (109/213)   \u001b[K\rremote: Counting objects:  52% (111/213)   \u001b[K\rremote: Counting objects:  53% (113/213)   \u001b[K\rremote: Counting objects:  54% (116/213)   \u001b[K\rremote: Counting objects:  55% (118/213)   \u001b[K\rremote: Counting objects:  56% (120/213)   \u001b[K\rremote: Counting objects:  57% (122/213)   \u001b[K\rremote: Counting objects:  58% (124/213)   \u001b[K\rremote: Counting objects:  59% (126/213)   \u001b[K\rremote: Counting objects:  60% (128/213)   \u001b[K\rremote: Counting objects:  61% (130/213)   \u001b[K\rremote: Counting objects:  62% (133/213)   \u001b[K\rremote: Counting objects:  63% (135/213)   \u001b[K\rremote: Counting objects:  64% (137/213)   \u001b[K\rremote: Counting objects:  65% (139/213)   \u001b[K\rremote: Counting objects:  66% (141/213)   \u001b[K\rremote: Counting objects:  67% (143/213)   \u001b[K\rremote: Counting objects:  68% (145/213)   \u001b[K\rremote: Counting objects:  69% (147/213)   \u001b[K\rremote: Counting objects:  70% (150/213)   \u001b[K\rremote: Counting objects:  71% (152/213)   \u001b[K\rremote: Counting objects:  72% (154/213)   \u001b[K\rremote: Counting objects:  73% (156/213)   \u001b[K\rremote: Counting objects:  74% (158/213)   \u001b[K\rremote: Counting objects:  75% (160/213)   \u001b[K\rremote: Counting objects:  76% (162/213)   \u001b[K\rremote: Counting objects:  77% (165/213)   \u001b[K\rremote: Counting objects:  78% (167/213)   \u001b[K\rremote: Counting objects:  79% (169/213)   \u001b[K\rremote: Counting objects:  80% (171/213)   \u001b[K\rremote: Counting objects:  81% (173/213)   \u001b[K\rremote: Counting objects:  82% (175/213)   \u001b[K\rremote: Counting objects:  83% (177/213)   \u001b[K\rremote: Counting objects:  84% (179/213)   \u001b[K\rremote: Counting objects:  85% (182/213)   \u001b[K\rremote: Counting objects:  86% (184/213)   \u001b[K\rremote: Counting objects:  87% (186/213)   \u001b[K\rremote: Counting objects:  88% (188/213)   \u001b[K\rremote: Counting objects:  89% (190/213)   \u001b[K\rremote: Counting objects:  90% (192/213)   \u001b[K\rremote: Counting objects:  91% (194/213)   \u001b[K\rremote: Counting objects:  92% (196/213)   \u001b[K\rremote: Counting objects:  93% (199/213)   \u001b[K\rremote: Counting objects:  94% (201/213)   \u001b[K\rremote: Counting objects:  95% (203/213)   \u001b[K\rremote: Counting objects:  96% (205/213)   \u001b[K\rremote: Counting objects:  97% (207/213)   \u001b[K\rremote: Counting objects:  98% (209/213)   \u001b[K\rremote: Counting objects:  99% (211/213)   \u001b[K\rremote: Counting objects: 100% (213/213)   \u001b[K\rremote: Counting objects: 100% (213/213), done.\u001b[K\n","remote: Compressing objects: 100% (156/156), done.\u001b[K\n","remote: Total 213 (delta 86), reused 173 (delta 46), pack-reused 0\u001b[K\n","Receiving objects: 100% (213/213), 1.92 MiB | 6.05 MiB/s, done.\n","Resolving deltas: 100% (86/86), done.\n"],"name":"stdout"}]},{"metadata":{"id":"r1eC9K1GCDkG","colab_type":"code","outputId":"0134298b-a504-4ea8-f89b-93d8fce1632c","executionInfo":{"status":"error","timestamp":1555473631828,"user_tz":-330,"elapsed":2985,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":1424}},"cell_type":"code","source":["# Building CNN based on AlexNet Architecture\n","\n","# Importing Keras libraries and packages\n","from keras.models import Sequential\n","from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n","from keras.layers.normalization import BatchNormalization\n","from keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint\n","\n","# Initializing the CNN\n","classifier = Sequential()\n","\n","# Convolution Step 1\n","classifier.add(Convolution2D(96, 11, strides = (4, 4), padding = 'valid', input_shape=(224, 224, 3), activation = 'relu'))\n","\n","# Max Pooling Step 1\n","classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n","classifier.add(BatchNormalization())\n","\n","# Convolution Step 2\n","classifier.add(Convolution2D(256, 11, strides = (1, 1), padding='valid', activation = 'relu'))\n","\n","# Max Pooling Step 2\n","classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding='valid'))\n","classifier.add(BatchNormalization())\n","\n","# Convolution Step 3\n","classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n","classifier.add(BatchNormalization())\n","\n","# Convolution Step 4\n","classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n","classifier.add(BatchNormalization())\n","\n","# Convolution Step 5\n","classifier.add(Convolution2D(256, 3, strides=(1,1), padding='valid', activation = 'relu'))\n","\n","# Max Pooling Step 3\n","classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n","classifier.add(BatchNormalization())\n","\n","# Flattening Step\n","classifier.add(Flatten())\n","\n","# Full Connection Step\n","classifier.add(Dense(units = 4096, activation = 'relu'))\n","classifier.add(Dropout(0.4))\n","classifier.add(BatchNormalization())\n","classifier.add(Dense(units = 4096, activation = 'relu'))\n","classifier.add(Dropout(0.4))\n","classifier.add(BatchNormalization())\n","classifier.add(Dense(units = 1000, activation = 'relu'))\n","classifier.add(Dropout(0.2))\n","classifier.add(BatchNormalization())\n","classifier.add(Dense(units = 3, activation = 'softmax'))\n","classifier.summary()\n","\n","\n","# Compiling the CNN\n","# classifier.compile(optimizer='adam',\n","#                    loss='categorical_crossentropy',\n","#                    metrics=['accuracy'])\n","\n","\n","# Compiling the CNN\n","classifier.compile(optimizer=optimizers.SGD(lr=0.001, momentum=0.9, decay=0.005),\n","                   loss='categorical_crossentropy',\n","                   metrics=['accuracy'])\n","\n","\n","\n","# image preprocessing\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2,\n","                                   width_shift_range=0.2,\n","                                   height_shift_range=0.2,\n","                                   rotation_range=40,\n","                                   horizontal_flip=True,\n","                                   fill_mode='nearest')\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","batch_size = 32\n","\n","train_data_dir = \"\"     # directory of training data\n","\n","test_data_dir = \"/content/gdrive/My Drive/AlexNetModel.hdf5\"      # directory of test data\n","\n","# training_set = train_datagen.flow_from_directory(train_data_dir,\n","#                                                  target_size=(224, 224),\n","#                                                  batch_size=batch_size,\n","#                                                  class_mode='categorical')\n","\n","test_set = test_datagen.flow_from_directory(test_data_dir,\n","                                            target_size=(224, 224),\n","                                            batch_size=batch_size,\n","                                            class_mode='categorical')\n","\n","print(training_set.class_indices)\n","\n","\n","# checkpoint\n","weightpath = \"/content/gdrive/My Drive/AlexNetModel.hdf5\"\n","checkpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","\n","\n","#fitting images to CNN\n","history = classifier.fit_generator(training_set,\n","                         steps_per_epoch=training_set.samples//batch_size,\n","                         validation_data=test_set,\n","                         epochs=50,\n","                         validation_steps=test_set.samples//batch_size,\n","                         callbacks=callbacks_list)\n","\n","\n","#fitting images to CNN\n","# history = classifier.fit_generator(training_set,\n","#                                    steps_per_epoch=training_set.samples//batch_size,\n","#                                    validation_data=test_set,\n","#                                    epochs=50,\n","#                                    validation_steps=test_set.samples//batch_size)\n","\n","\n","# #saving model\n","# filepath=\"/content/gdrive/My Drive/AlexNetModel.hdf5\"\n","# classifier.save(filepath)\n","\n","#plotting training values\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(loss) + 1)\n","\n","#accuracy plot\n","plt.plot(epochs, acc, color='green', label='Training Accuracy')\n","plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","\n","#loss plot\n","plt.plot(epochs, loss, color='pink', label='Training Loss')\n","plt.plot(epochs, val_loss, color='red', label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_6 (Conv2D)            (None, 54, 54, 96)        34944     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 27, 27, 96)        0         \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 27, 27, 96)        384       \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 17, 17, 256)       2973952   \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 8, 8, 256)         1024      \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 6, 6, 384)         885120    \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 6, 6, 384)         1536      \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 4, 4, 384)         1327488   \n","_________________________________________________________________\n","batch_normalization_12 (Batc (None, 4, 4, 384)         1536      \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 2, 2, 256)         884992    \n","_________________________________________________________________\n","max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n","_________________________________________________________________\n","batch_normalization_13 (Batc (None, 1, 1, 256)         1024      \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 256)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 4096)              1052672   \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 4096)              0         \n","_________________________________________________________________\n","batch_normalization_14 (Batc (None, 4096)              16384     \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 4096)              16781312  \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 4096)              0         \n","_________________________________________________________________\n","batch_normalization_15 (Batc (None, 4096)              16384     \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 1000)              4097000   \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 1000)              0         \n","_________________________________________________________________\n","batch_normalization_16 (Batc (None, 1000)              4000      \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 3)                 3003      \n","=================================================================\n","Total params: 28,082,755\n","Trainable params: 28,061,619\n","Non-trainable params: 21,136\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-13b53fc450a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m                                             \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                                             class_mode='categorical')\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         )\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/AlexNetModel.hdf5'"]}]},{"metadata":{"id":"yn6fZNAWBhwI","colab_type":"code","outputId":"312cb6db-a735-4113-ee50-f1bff66436fc","executionInfo":{"status":"ok","timestamp":1555473705782,"user_tz":-330,"elapsed":65183,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"Eye2z0zClcc6","colab_type":"code","outputId":"7182e654-cb4b-4239-c5cf-5b703cb0da15","executionInfo":{"status":"ok","timestamp":1555473714031,"user_tz":-330,"elapsed":3830,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["drive  Plant-Diseases-Recognition  sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"T-MAxOWvldIm","colab_type":"code","outputId":"b23f2bb4-a74c-48e3-8118-0c20bb837d2d","executionInfo":{"status":"ok","timestamp":1555473718335,"user_tz":-330,"elapsed":1620,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["%cd \"drive/My Drive\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"metadata":{"id":"UVWER6KHlx7n","colab_type":"code","outputId":"44d6b9fc-1053-414a-d704-e684b4cdf67b","executionInfo":{"status":"ok","timestamp":1555473729183,"user_tz":-330,"elapsed":4112,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!pwd"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"metadata":{"id":"l6R81l3Flzx-","colab_type":"code","outputId":"1fed4438-e893-4b8b-b65f-d8ad979cb78d","executionInfo":{"status":"ok","timestamp":1555473735017,"user_tz":-330,"elapsed":4345,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"cell_type":"code","source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":[" 1.c\n"," 23380051308101_report.pdf\n","'affidavit for income certificate.pdf'\n"," Affidavit.pdf\n"," AlexNetModel.hdf5\n"," application.docx\n"," application.gdoc\n","'bonafite certificate.pdf'\n"," BT16CSE_020.pdf\n"," BT16CSE020.pdf\n","'Colab Notebooks'\n","'Copy of photo.pdf'\n","'Dhamendra Singh Negi_CV - dharmendra negi.pdf'\n","'Dhamendra Singh Negi_CV.pdf'\n","'Dharmendra Singh Negi__BT16CSE020.pdf'\n","'Dharmendra Singh Negi Docs'\n"," DS_Negi.pdf\n","'Google Photos'\n","'hard copy perfect.pptx'\n"," INCOME_CERTIFICATE_AFFIDAVIT.pdf\n"," INCOME.pdf\n","'minor project report.gdoc'\n","'New Doc 2017-09-20 (1).pdf'\n"," Plant-Diseases-Recognition\n","'Residence identity.pdf'\n"," se_function_point_metric.gdoc\n"," se_function_point_metric.odt\n","'sohanPart (1).gdoc'\n","'sohanPart (2).gdoc'\n","'sohanPart (3).gdoc'\n"," sohanPart.gdoc\n"," sohanPart.odt\n","'This is to certify that Mr.docx'\n"],"name":"stdout"}]},{"metadata":{"id":"Vg4Plr6nmK1u","colab_type":"code","outputId":"effffa17-b6c4-4ac7-e137-9568bb9d4223","executionInfo":{"status":"ok","timestamp":1555475577074,"user_tz":-330,"elapsed":75995,"user":{"displayName":"dharmendra negi","photoUrl":"https://lh6.googleusercontent.com/-2rvK3KVcMPU/AAAAAAAAAAI/AAAAAAAAAFY/a0nLNg0KTvA/s64/photo.jpg","userId":"10784398794502577852"}},"colab":{"base_uri":"https://localhost:8080/","height":7280,"output_embedded_package_id":"1cNRZ1G0J3ZEry7_ilt6U-ZocVMOufIUT"}},"cell_type":"code","source":["%matplotlib inline\n","import tensorflow as tf\n","import numpy as np\n","from keras.models import load_model\n","from keras.preprocessing import image\n","import matplotlib.pyplot as plt\n","import os\n","# global graph, model, output_list\n","\n","def predict(myfile):\n","    img = image.load_img(myfile, target_size=(224, 224))\n","    plt.imshow(img)\n","    plt.show()\n","    img = image.img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    img = img/255\n","\n","\n","\n","    with graph.as_default():\n","        prediction = model.predict(img)\n","    prediction_flatten = prediction.flatten()\n","    max_val_index = np.argmax(prediction_flatten)\n","    result = output_list[max_val_index]\n","    print(result)\n","\n","\n","graph = tf.get_default_graph()\n","model = load_model('/content/drive/My Drive/AlexNetModel.hdf5')\n","\n","output_dict = {'Apple___Apple_scab': 0,\n","               'Apple___Black_rot': 1,\n","               'Apple___Cedar_apple_rust': 2,\n","               'Apple___healthy': 3,\n","               'Blueberry___healthy': 4,\n","               'Cherry_(including_sour)___Powdery_mildew': 5,\n","               'Cherry_(including_sour)___healthy': 6,\n","               'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7,\n","               'Corn_(maize)___Common_rust_': 8,\n","               'Corn_(maize)___Northern_Leaf_Blight': 9,\n","               'Corn_(maize)___healthy': 10,\n","               'Grape___Black_rot': 11,\n","               'Grape___Esca_(Black_Measles)': 12,\n","               'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13,\n","               'Grape___healthy': 14,\n","               'Orange___Haunglongbing_(Citrus_greening)': 15,\n","               'Peach___Bacterial_spot': 16,\n","               'Peach___healthy': 17,\n","               'Pepper,_bell___Bacterial_spot': 18,\n","               'Pepper,_bell___healthy': 19,\n","               'Potato___Early_blight': 20,\n","               'Potato___Late_blight': 21,\n","               'Potato___healthy': 22,\n","               'Raspberry___healthy': 23,\n","               'Soybean___healthy': 24,\n","               'Squash___Powdery_mildew': 25,\n","               'Strawberry___Leaf_scorch': 26,\n","               'Strawberry___healthy': 27,\n","               'Tomato___Bacterial_spot': 28,\n","               'Tomato___Early_blight': 29,\n","               'Tomato___Late_blight': 30,\n","               'Tomato___Leaf_Mold': 31,\n","               'Tomato___Septoria_leaf_spot': 32,\n","               'Tomato___Spider_mites Two-spotted_spider_mite': 33,\n","               'Tomato___Target_Spot': 34,\n","               'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35,\n","               'Tomato___Tomato_mosaic_virus': 36,\n","               'Tomato___healthy': 37}\n","\n","output_list = list(output_dict.keys())\n","\n","IMAGE_DIR = \"/content/drive/My Drive/Plant-Diseases-Recognition/plant_diseases/plant_app/static/plant_app/images/\"\n","\n","\n","leaf_images = os.listdir(IMAGE_DIR)\n","#print(leaf_images)\n","\n","for leaf_image in leaf_images:\n","    try:\n","        predict(IMAGE_DIR+leaf_image)\n","    except Exception as e:\n","        pass"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}